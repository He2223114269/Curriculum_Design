##  [划分数据集.ipynb](划分数据集.ipynb)  



1. **封装函数**，在加载音频对`MP3`文件进行文件转换为`wav`文件，并对`wav`文件只提取前`10s`，读取数据后转换为列表设置采样率统一为`220500`进行归一化，最终转化为`numpy`数组。

2. **封装函数**，进行MFCC特征提取 ==参数数据，采样率==， 其中MFCC系数选取为`40`，获取信息多，噪声也多。返回值二维数组，列为每一帧，行为每一系数，后使用`numpy`对每帧的`nfcc`系数取平均值，得到` 1 * `维数据（？ MFCC的变种吗？）
3. 循环遍历每个文件收集数据，并记录耗时，划分训练集`0.8` ，在划分测试集的基础上，在测试集再次划分出验证集，这里比例是（0.8 ，0.16，0.04)，此外师兄还在文件上额外引入验证集和测试集加大数据集量。



> tips: 
>
> 训练集、测试集和验证集都是在机器学习和深度学习中用于评估模型性能的数据集。
>
> 训练集是用来训练模型的数据集。模型通过学习训练集中的样本，可以调整其参数和权重来最小化损失函数，以便对新的未见过的数据做出准确的预测。
>
> 测试集是用来评估模型性能的数据集。在模型训练完成之后，使用测试集来测试模型对未见过的数据的泛化能力，以便评估模型的性能。
>
> 验证集是用来验证模型在训练过程中的表现的数据集。在训练模型时，使用验证集来调整模型的超参数，例如学习率、正则化强度等，以避免模型过拟合。
>
> 在使用训练集、测试集和验证集时，通常会采用一定比例的划分。例如，可以将数据集划分为训练集、测试集和验证集三部分，比例为70%、20%、10%。也可以将数据集划分为训练集和测试集两部分，比例为80%、20%。这些比例的选择取决于具体的任务和数据集的大小。通常，大型数据集可以使用更大的测试集和验证集比例，而小型数据集则需要更小的比例以确保模型的准确性。
>
> 验证集是在机器学习中用于评估模型性能和进行超参数调整的重要工具。以下是使用验证集的一些步骤：
>
> 1. 数据集划分：将数据集分为训练集、验证集和测试集。通常将数据集的70％用于训练，15％用于验证，15％用于测试。
> 2. 训练模型：使用训练集训练模型。训练过程中可以使用交叉验证等技术进行模型选择和调整超参数。
> 3. 验证模型：使用验证集评估模型性能，如准确率、精度、召回率等指标。
> 4. 调整超参数：根据验证集的评估结果，调整超参数，如学习率、正则化强度、模型结构等，以提高模型性能。
> 5. 测试模型：最终，使用测试集评估模型的性能，以确保模型可以泛化到新数据上。
>
> 在使用验证集时，需要注意以下几点：
>
> - 验证集的规模应该足够大，以确保评估结果的可靠性。
> - 验证集应该与训练集和测试集独立，以避免模型在验证集上过拟合。
> - 在进行模型选择和超参数调整时，应该始终使用验证集进行评估，而不是测试集，以避免过拟合。
> - 验证集的评估结果不应该被过度依赖，应该与其他评估指标一起使用，以获得更全面的模型性能评估。

##  [Untitled.ipynb](Untitled.ipynb) 

1. 该代码首先分析了数据集整体个数，并绘制直方图展示

2. 读取一段音频，听取录音，绘图分析，分别展示了`正常音频、空白音频、纯噪音音频`。

3. 绘制语谱图比较

4. **封装函数**，封装了能够产生特定信噪比的加噪数据，SNR(`signal noise ratio`)，为$SNR = 10*log_{10}(Ps/Pn)$，根据该公式得到$ Pn=\sqrt{\frac{S^2}{10^{\frac{SNR}{10}}}}$，高斯分布（0~1）乘上该噪声强度。获取指定噪声强度的信号 

5. **封装函数**，音量增强，这一段代码计算了一个标量（scalar），以便将输入信号的音量增益到所需的分贝级别。它使用了dB参数表示的增益量，并且通过wav参数传递的音频信号来计算输入信号的平均功率（wav_p）。

   在计算标量时，使用了以下公式：

   $scalar = sqrt(10**(dB/10) / (wav_p + eps))$

   其中，dB/10将dB值转换为增益因子，10的幂将增益因子转换为增益比，sqrt表示平方根函数，eps是一个非常小的数，用于避免除以零错误。公式的目的是计算一个标量，使得将输入信号乘以该标量后，其平均功率就会增益到所需的分贝级别。这是因为声音的感知是与其振幅的对数有关的。即使信号的振幅增加了很多，人耳也可能感觉声音没有变大。

   > 音量增强后，幅度增大了很多，但是声音大小没有变化这是因为人耳的听觉系统是非线性的，它对于相同的物理量级，其感知的响度并不是线性的。
   >
   > 例如，如果一个信号的振幅增加了10倍，那么它的功率就会增加100倍。然而，人耳对于100倍的功率增益可能只会感知到大约2倍左右的响度增益。因此，即使信号的振幅增加了很多，人耳也可能感觉声音没有变化。
   >
   > 在进行音量增强时，应该考虑到这个非线性的听觉响度特性。在实际的应用中，可以使用一些特定的算法来增强音频信号的响度，以使其在听觉上更容易察觉到响度的变化。

   

6. `pitch shift`，`time stretch` 对语音信号进行音调转换和时间伸缩。并展示了

   > 6.1 `pitch_shift`函数用于改变语音信号的音调，可以通过指定n_steps参数来控制转换的大小，n_steps为负数表示音调降低，为正数表示音调升高。具体而言，该函数将输入语音信号的频率轴上的**所有频率值乘以2的n_steps/12次方幂，从而实现音调的变化**。音调是声音的一个基本特征，指的是声音的高低音高。它由声波的频率决定，频率越高则音调越高，频率越低则音调越低。在音乐中，音调是指一种音符的高低，不同音调的音符在音高上相差一个半音或全音。在语音中，音调也常常被称为声调，是指一句话或一个单词中某个音节的高低声调，语言中的音调对于语义的表达和理解都有着重要的影响。
   >
   > wav_data：输入的语音信号。
   >
   > sr：采样率。
   >
   > n_steps：音调转换的大小，单位为半音（semitones），默认为-3，表示将音调降低3个半音。
   >
   > 6.2 `time_stretch`函数用于改变语音信号的持续时间，可以通过指定rate参数来控制时间伸缩的大小，rate大于1表示时间伸长，rate小于1表示时间缩短。具体而言，**该函数将输入语音信号的时域轴上的采样点的时间间隔缩放为原来的1/rate倍**，从而实现时间伸缩。
   >
   > wav_data：输入的语音信号。
   >
   > rate：时间伸缩的大小，大于1表示时间伸长，小于1表示时间缩短，默认为1.2，表示将时间伸长20%。

   一般使用目的是

   1. 语音识别：音调转换和时间伸缩可以用于语音识别中，以增强识别的准确性。例如，在噪声环境下，将输入的语音信号进行音调转换或时间伸缩，可以使语音信号更易于识别。
   2. 数据增强：**音调转换和时间伸缩也常用于数据增强的技术中，以扩展训练数据集**，提高模型的鲁棒性和泛化能力。

   需要注意的是，音调转换和时间伸缩都会对信号的质量和特征造成一定的影响，因此应该根据具体的应用场景和要求来选择合适的处理方式和参数，以保证处理后的信号质量和特征符合要求。
   
7. 使用了 Python 音频处理库 `Soundpy `中的 `augment.harmonic_distortion` 函数，用于对给定的音频数据进行谐波失真处理。具体来说，函数会按照给定的采样率（`sr`）对音频数据（`wav_data`）进行谐波失真处理，并返回处理后的音频数据。谐波失真处理是一种经典的音频效果，它**可以增加音乐的明亮度和噪声，同时也可以使音乐更有动态感。**

   该函数的具体实现方式可能会因不同库的实现而略有差异，但通常会基于非线性函数对原始音频数据进行修改以实现谐波失真的效果。

   > 谐波失真是指在音频信号处理中，当一个信号被放大时，原本不存在的谐波成分被引入到信号中，导致信号的谐波失真。通俗地说，谐波失真会让原本清晰的音频信号变得杂乱无章、失真严重，影响人们的听觉体验。谐波失真在音频放大器、麦克风等音频处理设备中都有可能发生。谐波是指在振动过程中，频率为基频整数倍的波形分量。例如，当一根琴弦振动时，它不仅产生基频振动，还可能同时产生频率是基频的2倍、3倍、4倍等整数倍的波形分量，这些波形分量就是谐波。在音乐中，各种乐器演奏时产生的复杂声音都包含着不同频率的谐波成分，这些成分的存在使得不同乐器演奏同一音高时所产生的声音是有所不同的，从而呈现出各具特色的音色。
   >
   >  `augment.harmonic_distortion`函数是音频数据增强的一种方法，通过引入谐波失真来改变音频信号的音色，增加音频数据的多样性。
   >
   > 谐波失真是指在原始音频信号中添加一定比例的音频信号的高次谐波分量，从而使音频信号产生更丰富的频谱。在 `augment.harmonic_distortion`函数中，通过控制添加谐波分量的比例和强度来实现谐波失真的处理。具体来说，该函数会计算原始音频信号的快速傅里叶变换（FFT），然后在FFT输出中添加谐波分量，最后通过反向FFT得到谐波失真处理后的音频信号。
   >
   > 这种谐波失真处理方法可以改变音频信号的音色，从而增加数据的多样性，使得模型训练更加健壮。

8. **封装函数** `time_stretch` 封装定义时间偏移，偏移数据，反过来便是偏移时间

9. 语速调整，升高采样率



> tips: 注意语谱图的参数，单边谱还是双边谱，等一些默认参数
>
> 实验中发现的问题之一：
>
> 1. 使用不同函数所导入的数据长度不同，对应采样率也不同，但是播放都是一样的效果，而`librose`则固定为22050.  
>
>    >  这是因为导入音频数据时使用的采样率通常是根据音频文件的元数据中包含的信息来确定的。例如，对于WAV文件，采样率通常存储在文件头中，可以通过读取文件头来确定。而对于其他音频格式，则可能需要使用特定的库来读取元数据并确定采样率。
>    >
>    > 在使用`Librosa`导入音频数据时，默认采样率是22.05kHz，这是因为22.05kHz是CD音频的采样率之一，而`Librosa`旨在方便地处理和分析音频数据。但是，我们可以使用`sr`参数来指定所需的采样率，以避免因低采样率而导致的失真。
>    >
>    > 在实践中，根据采样定律，采样率应该至少是信号最高频率的两倍。因此，对于具有高频分量的音频信号，使用较低的采样率可能会导致失真或损失。因此，在处理音频数据时，应该根据信号特性和需求选择合适的采样率，以避免不必要的信号失真。
>
>    
>
> 2. 不同数据所绘制的语谱图不同，其中还可能出现为空的情况，这是可以观察到，数据为空的状况，为正常状况 
>
> ![image-20230316212440523](代码总结整理.assets/image-20230316212440523.png)

##  [语音分割.ipynb](语音分割.ipynb) 

1. 该代码提取文件，正则表达式模式是用来匹配以".wav"结尾的字符串，并将".wav"之前的任意字符作为匹配结果。其中"{1,}"表示匹配前面的任意字符至少1次，"."表示匹配任意字符，"()"表示匹配结果的组，迭代处理 positive 列表中的每个音频文件
2. `split_on_silence`其中的静默部分设置为为0.5秒的部分作为分割点，音量阈值为-35分贝，并在分割后的音频段之间保留0.3秒的静默，返回分割后的音频段列表。并对结果列表为0的音频存贮在无效列表中存贮。（参数是通过测试过的）
3. 对分割好的数据**逆序循环** (因为使用pop函数去除，所以避免长度改变导致删除跳过）去有效分段（大于0.6s，小于10s)，最后存在`原文件名+chunks{i} `的文件中，还应该考虑讲无效的音频也放到这个文件夹中。



> tips:
>
> 以下是一些经典常用的用于语音处理的库：
>
> 1. `librosa`：用于分析和处理音频信号的Python库，支持许多常见的音频格式。
> 2. `Pydub`：一个简单易用的Python库，用于处理音频文件，可以读取和写入多种格式的音频文件。
> 3. `SpeechRecognition`：一个支持多种语音识别引擎的Python库，可以将音频转换为文本。
> 4. `Soundfile`：用于读取和写入许多常见音频格式的Python库，具有简单易用的API。
> 5. ``NumPy``：用于数值计算和科学计算的Python库，可以用于处理音频信号数据。
> 6. `SciPy：Python`中的另一个科学计算库，提供了许多用于信号处理的函数。
> 7. `TensorFlow：Google`开发的深度学习框架，支持语音识别、音频处理等应用。
> 8. `Kaldi`：一个流行的开源语音识别工具包，用于训练和测试语音识别模型。
> 9. `HTK：Hidden `Markov Model Toolkit，另一个流行的语音识别工具包，用于语音信号的特征提取、建模和识别。
> 10. `Praat`：一个用于语音分析的软件，可以进行语音信号的基本处理和分析，例如提取基频、共振峰等。
